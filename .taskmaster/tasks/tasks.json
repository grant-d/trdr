{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Project Setup and Core Interfaces",
        "description": "Initialize the TypeScript project with necessary dependencies and implement core interfaces for the data pipeline.",
        "details": "1. Initialize a new TypeScript project with npm\n2. Configure TypeScript, ESLint, and testing framework (node:test)\n3. Set up Winston for structured logging\n4. Implement core interfaces:\n   - `OhlcvDto` interface for market data\n   - `DataProvider` interface for data sources\n   - `Transform` interface for pipeline transformations\n   - Basic pipeline executor\n5. Create project directory structure\n\n```typescript\n// Example core interfaces implementation\ninterface OhlcvDto {\n  exchange: string;\n  symbol: string;\n  timestamp: number; // UTC unix timestamp in milliseconds\n  open: number;\n  high: number;\n  low: number;\n  close: number;\n  volume: number;\n  [key: string]: number | string; // For additional columns added by transforms\n}\n\ninterface DataProvider {\n  name: string;\n  connect(): Promise<void>;\n  disconnect(): Promise<void>;\n  getHistoricalData(params: HistoricalParams): AsyncIterator<OhlcvDto>;\n  subscribeRealtime(params: RealtimeParams): AsyncIterator<OhlcvDto>;\n  getRequiredEnvVars(): string[];\n}\n\ninterface Transform {\n  type: TransformType;\n  apply(data: AsyncIterator<OhlcvDto>): AsyncIterator<OhlcvDto>;\n  getCoefficients(): Record<string, number> | null;\n}\n```",
        "testStrategy": "1. Unit tests for each interface implementation\n2. Verify TypeScript compilation works correctly\n3. Test logging configuration with different log levels\n4. Validate project structure follows best practices\n5. Ensure ESLint rules are properly enforced",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize Project with npm and TypeScript",
            "description": "Set up the project with npm, install TypeScript, and configure the TypeScript compiler options.",
            "dependencies": [],
            "details": "1. Create a new directory for the project\n2. Run `npm init` to create package.json\n3. Install TypeScript: `npm install typescript --save-dev`\n4. Install ts-node for development: `npm install ts-node --save-dev`\n5. Create tsconfig.json with appropriate settings:\n   - Set target to ES2020\n   - Enable strict type checking\n   - Configure module resolution\n   - Set output directory to ./dist\n6. Add npm scripts for build, start, and test\n7. Install essential dependencies: `npm install commander chalk inquirer`\n8. Create .gitignore file with appropriate entries\n\nAcceptance Criteria:\n- Project initializes without errors\n- TypeScript compiles successfully\n- Package.json contains all necessary scripts and dependencies\n- tsconfig.json is properly configured",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Set up Logging with Winston",
            "description": "Implement a logging system using Winston to handle different log levels and output formats.",
            "dependencies": [
              1
            ],
            "details": "1. Install Winston: `npm install winston --save`\n2. Create a logger module in src/utils/logger.ts\n3. Configure Winston with the following features:\n   - Multiple transport options (console, file)\n   - Different log levels (error, warn, info, debug)\n   - Custom formatting with timestamps\n   - Log rotation for file logs\n4. Create a simple API for the application to use:\n   - logger.error()\n   - logger.warn()\n   - logger.info()\n   - logger.debug()\n5. Add configuration options to control log verbosity\n\nAcceptance Criteria:\n- Logger successfully outputs to console and file\n- Different log levels work correctly\n- Log messages include timestamps and appropriate formatting\n- Log files rotate properly when size limits are reached\n- Logger can be easily imported and used throughout the application",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement OhlcvDto Interface",
            "description": "Create the OhlcvDto (Open, High, Low, Close, Volume) interface to standardize market data representation across the application.",
            "dependencies": [
              1
            ],
            "details": "1. Create src/models/ohlcv.dto.ts file\n2. Define the OhlcvDto interface with the following properties:\n   - timestamp: number (Unix timestamp in milliseconds)\n   - open: number\n   - high: number\n   - low: number\n   - close: number\n   - volume: number\n3. Add optional properties that might be useful:\n   - symbol?: string\n   - interval?: string\n4. Create utility functions for OhlcvDto:\n   - isValid(): boolean - to validate data integrity\n   - toString(): string - for logging and display\n5. Add documentation comments for each property and method\n\nAcceptance Criteria:\n- OhlcvDto interface is properly typed with all required fields\n- Interface includes appropriate documentation\n- Utility functions work correctly\n- Interface can be imported and used in other modules",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement DataProvider and Transform Interfaces",
            "description": "Create the core interfaces for data providers and data transformations that will be used throughout the application.",
            "dependencies": [
              3
            ],
            "details": "1. Create src/interfaces/data-provider.interface.ts:\n   - Define DataProvider interface with methods:\n     - fetchData(symbol: string, interval: string, limit?: number): Promise<OhlcvDto[]>\n     - getAvailableSymbols(): Promise<string[]>\n     - getAvailableIntervals(): string[]\n   - Include error handling specifications\n\n2. Create src/interfaces/transform.interface.ts:\n   - Define Transform interface with methods:\n     - apply(data: OhlcvDto[]): OhlcvDto[]\n     - getName(): string\n     - getDescription(): string\n     - getParameters(): Record<string, any>\n     - setParameters(params: Record<string, any>): void\n\n3. Add documentation for both interfaces explaining their purpose and usage\n\nAcceptance Criteria:\n- Both interfaces are properly typed and documented\n- Interfaces are extensible for future implementations\n- Method signatures include appropriate parameter and return types\n- Error handling is properly specified",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Create Project Directory Structure",
            "description": "Establish the complete directory structure for the project with placeholder files and README documentation.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "1. Create the following directory structure:\n   - src/\n     - commands/ (for CLI commands)\n     - models/ (for data models and DTOs)\n     - interfaces/ (for application interfaces)\n     - providers/ (for data provider implementations)\n     - transforms/ (for data transformation implementations)\n     - utils/ (for utility functions)\n     - config/ (for application configuration)\n     - index.ts (main entry point)\n   - tests/\n     - unit/\n     - integration/\n   - docs/\n   - scripts/\n\n2. Create README.md with:\n   - Project description\n   - Installation instructions\n   - Usage examples\n   - Development guidelines\n\n3. Add placeholder index.ts files in each directory to maintain structure\n\nAcceptance Criteria:\n- All directories are created with appropriate structure\n- README.md contains comprehensive documentation\n- Project structure follows best practices for TypeScript applications\n- Directory structure facilitates easy navigation and maintenance",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 2,
        "title": "File-Based Data Provider Implementation",
        "description": "Implement the FileProvider to handle CSV and Parquet files with configurable column mapping and streaming processing.",
        "details": "1. Create a `FileProvider` class implementing the `DataProvider` interface\n2. Implement streaming CSV reader that processes data in chunks\n3. Add support for configurable column mapping\n4. Implement Parquet file support using parquet-wasm or arrow\n5. Ensure all file operations are streaming to avoid memory issues\n6. Standardize timestamps to UTC milliseconds\n7. Add validation for OHLC relationships (high â‰¥ low, etc.)\n\n```typescript\nclass FileProvider implements DataProvider {\n  name = 'file';\n  private filePath: string;\n  private format: 'csv' | 'parquet';\n  private columnMapping: ColumnMapping;\n  \n  constructor(config: FileProviderConfig) {\n    this.filePath = config.path;\n    this.format = config.format || this.detectFormatFromExtension(config.path);\n    this.columnMapping = config.columnMapping || this.getDefaultColumnMapping();\n  }\n  \n  async connect(): Promise<void> {\n    // Validate file exists and is readable\n  }\n  \n  async disconnect(): Promise<void> {\n    // Close any open file handles\n  }\n  \n  async *getHistoricalData(params: HistoricalParams): AsyncIterator<OhlcvDto> {\n    // Create readable stream for file\n    // Process in chunks (e.g., 1000 rows at a time)\n    // Map columns according to columnMapping\n    // Validate and standardize data\n    // Yield standardized OhlcvDto objects\n  }\n  \n  async *subscribeRealtime(): AsyncIterator<OhlcvDto> {\n    throw new Error('FileProvider does not support real-time data');\n  }\n  \n  getRequiredEnvVars(): string[] {\n    return [];\n  }\n  \n  private detectFormatFromExtension(path: string): 'csv' | 'parquet' {\n    // Auto-detect format from file extension\n  }\n  \n  private getDefaultColumnMapping(): ColumnMapping {\n    // Return default column mapping\n  }\n}\n```",
        "testStrategy": "1. Unit tests with sample CSV and Parquet files\n2. Test with different column mappings\n3. Verify streaming works with large files (>100MB)\n4. Test error handling for invalid files\n5. Benchmark performance with different batch sizes\n6. Verify memory usage remains constant regardless of file size",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create FileProvider Base Class Structure",
            "description": "Design and implement the base FileProvider class that will serve as the foundation for all file-based data providers.",
            "dependencies": [],
            "details": "1. Create an abstract FileProvider class that implements the DataProvider interface\n2. Define common properties: filePath, chunkSize, encoding options\n3. Implement configuration methods for file access settings\n4. Create factory method for instantiating specific file format providers\n5. Define abstract methods that concrete implementations must provide\n6. Implement common utility methods for file existence checking and error handling\n7. Add logging capabilities for file operations\n\nAcceptance Criteria:\n- FileProvider class properly implements DataProvider interface\n- Configuration options are properly documented and validated\n- Factory method correctly returns appropriate provider based on file extension\n- Error handling for common file access issues is implemented",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement Streaming CSV Reader with Chunk Processing",
            "description": "Create a CSV implementation of the FileProvider that processes large files in chunks to minimize memory usage.",
            "dependencies": [
              1
            ],
            "details": "1. Extend FileProvider to create CSVFileProvider implementation\n2. Implement streaming read capability using Node.js streams\n3. Add configurable chunk size processing to control memory usage\n4. Implement pause/resume functionality for processing control\n5. Add header detection and automatic column mapping\n6. Create progress tracking for long-running operations\n7. Implement proper resource cleanup after processing\n\nAcceptance Criteria:\n- Successfully processes CSV files larger than available memory\n- Memory usage remains constant regardless of file size\n- Correctly handles CSV parsing edge cases (quoted fields, escaped characters)\n- Progress can be monitored during processing\n- Resources are properly released after completion or errors",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Add Configurable Column Mapping",
            "description": "Implement a flexible column mapping system that allows users to map source file columns to standardized field names.",
            "dependencies": [
              2
            ],
            "details": "1. Design a column mapping configuration schema\n2. Implement automatic mapping based on header names\n3. Support manual mapping through configuration files\n4. Add support for derived fields (calculated from other fields)\n5. Implement type conversion during mapping (string to number, date parsing)\n6. Add validation for required fields\n7. Create helper methods to generate mapping templates\n\nAcceptance Criteria:\n- Users can define custom mappings between source columns and target fields\n- Automatic mapping works for standard column names\n- Type conversion correctly handles common data types\n- Missing required fields generate appropriate errors\n- Mapping configuration can be saved and loaded",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement Parquet File Support",
            "description": "Extend the FileProvider to support reading from Parquet files, maintaining the same interface as the CSV provider.",
            "dependencies": [
              1
            ],
            "details": "1. Create ParquetFileProvider extending the base FileProvider\n2. Integrate with appropriate Parquet library for Node.js\n3. Implement streaming/chunked reading for Parquet files\n4. Handle Parquet-specific schema and type information\n5. Optimize column projection to only read required fields\n6. Implement proper resource management for Parquet readers\n7. Add performance optimizations for Parquet's columnar format\n\nAcceptance Criteria:\n- Successfully reads standard Parquet files\n- Maintains consistent memory usage for large files\n- Correctly handles Parquet data types and converts to appropriate JavaScript types\n- Provides same interface as CSV provider for consistent usage\n- Takes advantage of Parquet's columnar nature for performance when possible",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Standardize Timestamps and Implement Data Validation",
            "description": "Add data validation and timestamp standardization across all file providers to ensure data quality and consistency.",
            "dependencies": [
              2,
              4
            ],
            "details": "1. Implement configurable validation rules for each field\n2. Create standard timestamp parsing for multiple formats\n3. Add timezone handling and normalization\n4. Implement data cleansing functions (trimming, null handling)\n5. Create validation error collection and reporting\n6. Add support for custom validation functions\n7. Implement row filtering based on validation results\n\nAcceptance Criteria:\n- All timestamps are converted to a standard format regardless of input\n- Validation rules can be applied to any field\n- Invalid data is properly identified and reported\n- Custom validation functions can be added for complex rules\n- Timezone conversion works correctly across different source formats",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Write Comprehensive Tests for File Providers",
            "description": "Create a comprehensive test suite for all file provider implementations covering different file formats, configurations, and edge cases.",
            "dependencies": [
              3,
              5
            ],
            "details": "1. Create unit tests for each provider class\n2. Implement integration tests with sample files of various formats\n3. Add performance tests for large files\n4. Create tests for error conditions and recovery\n5. Test memory usage patterns during streaming operations\n6. Implement validation rule testing\n7. Create test fixtures for different file formats and configurations\n\nAcceptance Criteria:\n- Test coverage exceeds 90% for all file provider code\n- Tests include both small and large file examples\n- Edge cases are properly tested (malformed files, unexpected data)\n- Memory usage is verified to remain constant for large files\n- All supported file formats have dedicated test cases\n- CI pipeline includes all tests with appropriate timeouts for large file tests",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 3,
        "title": "Storage Layer Implementation",
        "description": "Implement the storage layer with repository pattern for SQLite, CSV, and Parquet outputs with proper indexing and streaming writes.",
        "details": "1. Create `OhlcvRepository` interface\n2. Implement concrete repositories for each output type:\n   - `SqliteRepository`\n   - `CsvRepository`\n   - `ParquetRepository`\n3. Set up SQLite schema with proper indexing for time-series data\n4. Implement batch inserts with immediate disk writes\n5. Add support for coefficient storage\n6. Implement append-only writes to prevent memory buildup\n\n```typescript\ninterface OhlcvRepository {\n  initialize(): Promise<void>;\n  appendBatch(data: OhlcvDto[]): Promise<void>;\n  getLastTimestamp(symbol: string): Promise<number | null>;\n  storeCoefficients(symbol: string, transform: string, coefficients: Record<string, number>): Promise<void>;\n  getCoefficients(symbol: string, transform: string): Promise<Record<string, number> | null>;\n  close(): Promise<void>;\n}\n\nclass SqliteRepository implements OhlcvRepository {\n  private db: Database;\n  private table: string;\n  private batchSize: number;\n  \n  constructor(config: SqliteConfig) {\n    this.db = new Database(config.database);\n    this.table = config.table;\n    this.batchSize = config.batchSize || 1000;\n  }\n  \n  async initialize(): Promise<void> {\n    // Create tables if they don't exist\n    // Set up indexes on timestamp, symbol\n    // Prepare statements for batch inserts\n  }\n  \n  async appendBatch(data: OhlcvDto[]): Promise<void> {\n    // Use transaction for batch insert\n    // Handle dynamic columns from transforms\n  }\n  \n  async getLastTimestamp(symbol: string): Promise<number | null> {\n    // Query for most recent timestamp for a symbol\n  }\n  \n  async storeCoefficients(symbol: string, transform: string, coefficients: Record<string, number>): Promise<void> {\n    // Store transform coefficients with timestamp range\n  }\n  \n  async getCoefficients(symbol: string, transform: string): Promise<Record<string, number> | null> {\n    // Retrieve most recent coefficients for symbol/transform\n  }\n  \n  async close(): Promise<void> {\n    // Close database connection\n  }\n}\n```",
        "testStrategy": "1. Unit tests for each repository implementation\n2. Test batch inserts with various sizes\n3. Verify proper indexing with query performance tests\n4. Test coefficient storage and retrieval\n5. Verify append-only behavior works correctly\n6. Test with large datasets to ensure memory usage remains constant",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define OhlcvRepository Interface",
            "description": "Create a comprehensive interface that defines the contract for all repository implementations, including methods for storing and retrieving OHLCV data and coefficients.",
            "dependencies": [],
            "details": "Create an OhlcvRepository interface with the following methods: 1) save(data: OhlcvData): Promise<void>, 2) saveMany(data: OhlcvData[]): Promise<void>, 3) getBetweenDates(symbol: string, startDate: Date, endDate: Date): Promise<OhlcvData[]>, 4) getBySymbol(symbol: string): Promise<OhlcvData[]>, 5) saveCoefficient(symbol: string, name: string, value: number): Promise<void>, 6) getCoefficient(symbol: string, name: string): Promise<number>. Include proper TypeScript types and documentation for each method.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement SqliteRepository",
            "description": "Create a SQLite implementation of the OhlcvRepository interface with optimized schema design and indexing for time-series data.",
            "dependencies": [
              1
            ],
            "details": "Implement SqliteRepository class that: 1) Creates tables for OHLCV data with columns for timestamp, open, high, low, close, volume, and symbol, 2) Creates a separate table for coefficients with symbol, name, and value columns, 3) Implements proper indexing on timestamp and symbol columns for efficient queries, 4) Uses prepared statements for all database operations, 5) Implements transaction support for batch operations, 6) Handles database connection management properly. Include migration functionality for schema updates.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement CsvRepository",
            "description": "Create a CSV-based implementation of the OhlcvRepository interface with streaming writes for memory efficiency.",
            "dependencies": [
              1
            ],
            "details": "Implement CsvRepository class that: 1) Organizes CSV files by symbol in a configurable directory structure, 2) Uses streaming writes for memory-efficient operations on large datasets, 3) Implements proper CSV header management, 4) Creates a separate CSV for coefficients, 5) Handles file locking for concurrent access, 6) Implements efficient date-based filtering for queries, 7) Provides configurable CSV formatting options (delimiters, etc.). Ensure proper error handling for file operations.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement ParquetRepository",
            "description": "Create a Parquet-based implementation of the OhlcvRepository interface optimized for columnar storage of time-series data.",
            "dependencies": [
              1
            ],
            "details": "Implement ParquetRepository class that: 1) Stores OHLCV data in Parquet format organized by symbol, 2) Implements proper Parquet schema with appropriate data types, 3) Uses compression settings optimized for time-series data, 4) Implements efficient column filtering for queries, 5) Handles coefficient storage in a separate Parquet file, 6) Provides configurable partitioning options for large datasets. Ensure compatibility with common Parquet readers and tools.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Add Coefficient Storage Functionality",
            "description": "Enhance all repository implementations with comprehensive coefficient storage capabilities for storing calculation results and metadata.",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "For each repository implementation: 1) Add methods to store and retrieve named coefficients associated with symbols, 2) Implement bulk coefficient operations, 3) Add versioning support for coefficients, 4) Implement coefficient metadata (timestamp, source, etc.), 5) Create efficient querying capabilities for coefficients, 6) Ensure proper type handling for different coefficient values. Include documentation on coefficient naming conventions and usage patterns.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Implement Batch Operations and Append-Only Writes",
            "description": "Optimize all repository implementations with batch operation support and append-only write patterns for performance and data integrity.",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "For each repository implementation: 1) Add efficient batch insert operations, 2) Implement append-only write patterns to prevent data corruption, 3) Add transaction support where applicable, 4) Implement proper error handling and rollback mechanisms, 5) Add performance monitoring and logging, 6) Create configurable batch size settings, 7) Implement memory-efficient streaming for large batch operations. Include performance benchmarks for different batch sizes.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Create Comprehensive Repository Tests",
            "description": "Develop a thorough test suite for all repository implementations to ensure consistent behavior and data integrity.",
            "dependencies": [
              5,
              6
            ],
            "details": "Create tests that: 1) Verify all interface methods across all implementations, 2) Test edge cases like empty datasets and invalid inputs, 3) Verify data integrity after write operations, 4) Test performance with large datasets, 5) Verify proper error handling, 6) Test concurrent access scenarios, 7) Verify proper date filtering, 8) Test coefficient storage and retrieval, 9) Verify batch operations and transactions, 10) Include integration tests with the actual storage backends. Use a test framework like Jest and implement proper test fixtures and cleanup.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 4,
        "title": "Core Transformations Implementation",
        "description": "Implement the essential data transformations including missing value handling, timeframe aggregation, and normalization.",
        "details": "1. Create base `Transform` abstract class\n2. Implement `MissingValueHandler` transform with forward fill, interpolation strategies\n3. Implement `TimeframeAggregator` for flexible OHLC aggregation\n4. Implement `Normalizer` transforms (log returns, z-score, min-max)\n5. Implement `PriceCalculations` (HLC3, OHLC4, typical price)\n6. Ensure all transforms process data in a streaming fashion\n7. Add coefficient tracking for reversible transforms\n\n```typescript\nabstract class BaseTransform implements Transform {\n  protected type: TransformType;\n  protected coefficients: Record<string, number> | null = null;\n  \n  constructor(type: TransformType) {\n    this.type = type;\n  }\n  \n  abstract async *apply(data: AsyncIterator<OhlcvDto>): AsyncIterator<OhlcvDto>;\n  \n  getCoefficients(): Record<string, number> | null {\n    return this.coefficients;\n  }\n}\n\nclass TimeframeAggregator extends BaseTransform {\n  private targetTimeframe: string;\n  private alignToMarketOpen: boolean;\n  private currentBars: Map<string, OhlcvDto> = new Map();\n  \n  constructor(params: TimeframeAggregationParams) {\n    super('timeframeAggregation');\n    this.targetTimeframe = params.targetTimeframe;\n    this.alignToMarketOpen = params.alignToMarketOpen || false;\n  }\n  \n  async *apply(data: AsyncIterator<OhlcvDto>): AsyncIterator<OhlcvDto> {\n    // Calculate target timeframe in milliseconds\n    // Process incoming bars\n    // Aggregate into target timeframe\n    // Emit completed bars\n    // Handle final incomplete bars\n  }\n  \n  private getTargetTimestamp(timestamp: number): number {\n    // Calculate aligned timestamp for the target timeframe\n  }\n}\n\nclass LogReturnsNormalizer extends BaseTransform {\n  private base: 'natural' | 'log10';\n  \n  constructor(params: LogReturnsParams) {\n    super('logReturns');\n    this.base = params.base || 'natural';\n  }\n  \n  async *apply(data: AsyncIterator<OhlcvDto>): AsyncIterator<OhlcvDto> {\n    let prevClose: Record<string, number> = {};\n    \n    for await (const bar of data) {\n      const symbol = bar.symbol;\n      if (prevClose[symbol] !== undefined && prevClose[symbol] > 0) {\n        const logReturn = this.base === 'natural'\n          ? Math.log(bar.close / prevClose[symbol])\n          : Math.log10(bar.close / prevClose[symbol]);\n        \n        // Create new bar with log return\n        const newBar = { ...bar, log_return: logReturn };\n        yield newBar;\n      } else {\n        // First bar, no return can be calculated\n        yield { ...bar, log_return: null };\n      }\n      \n      prevClose[symbol] = bar.close;\n    }\n  }\n}\n```",
        "testStrategy": "1. Unit tests for each transform with sample data\n2. Test edge cases (missing data, zero values, etc.)\n3. Verify timeframe aggregation works with various multiples\n4. Test reversibility of normalizations\n5. Benchmark performance with large datasets\n6. Verify memory usage remains constant during processing",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 5,
        "title": "CLI Interface and Configuration",
        "description": "Implement the command-line interface with JSON config loading, validation, and pipeline execution.",
        "details": "1. Create CLI entry point with command parsing\n2. Implement JSON config loading and validation\n3. Create pipeline factory to build transforms from config\n4. Add support for config overrides via command flags\n5. Implement progress indicators for long operations\n6. Add interactive mode (process once and exit) and server mode (continuous)\n\n```typescript\ninterface PipelineConfig {\n  name: string;\n  description?: string;\n  input: InputConfig;\n  transformations: TransformConfig[];\n  output: OutputConfig;\n  options?: OptionsConfig;\n}\n\nclass ConfigValidator {\n  validate(config: PipelineConfig): ValidationResult {\n    // Validate config structure\n    // Check for required fields\n    // Validate transform compatibility and ordering\n    // Return validation result with any errors\n  }\n}\n\nclass PipelineFactory {\n  createPipeline(config: PipelineConfig): Pipeline {\n    // Create provider from input config\n    // Create transforms from transformation configs\n    // Create repository from output config\n    // Return assembled pipeline\n  }\n}\n\nasync function main() {\n  // Parse command line arguments\n  const args = parseArgs(process.argv.slice(2));\n  \n  // Load config file\n  const configPath = args.config || 'pipeline.json';\n  const config = await loadConfig(configPath);\n  \n  // Apply command line overrides\n  if (args.override) {\n    applyOverrides(config, args.override);\n  }\n  \n  // Validate config\n  const validator = new ConfigValidator();\n  const validationResult = validator.validate(config);\n  if (!validationResult.valid) {\n    console.error('Invalid configuration:', validationResult.errors);\n    process.exit(1);\n  }\n  \n  // Create and execute pipeline\n  const factory = new PipelineFactory();\n  const pipeline = factory.createPipeline(config);\n  \n  if (config.options?.mode === 'server') {\n    // Run in continuous server mode\n    await pipeline.runContinuously();\n  } else {\n    // Run in interactive mode (once and exit)\n    await pipeline.run();\n  }\n}\n```",
        "testStrategy": "1. Unit tests for config validation\n2. Test with various sample config files\n3. Test command line override functionality\n4. Verify error messages are helpful and clear\n5. Test progress indicators with long-running operations\n6. Integration tests for full pipeline execution",
        "priority": "medium",
        "dependencies": [
          1,
          2,
          3,
          4
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 6,
        "title": "Coinbase Provider Implementation",
        "description": "Implement the Coinbase provider with REST and WebSocket support, rate limiting, and reconnection logic.",
        "details": "1. Create `CoinbaseProvider` class implementing the `DataProvider` interface\n2. Implement REST API client for historical data\n3. Implement WebSocket client for real-time data\n4. Add rate limiting with exponential backoff\n5. Implement reconnection logic with circuit breaker pattern\n6. Add authentication via environment variables\n7. Implement backfill mechanism for gaps\n\n```typescript\nclass CoinbaseProvider implements DataProvider {\n  name = 'coinbase';\n  private apiKey: string;\n  private apiSecret: string;\n  private wsClient: WebSocket | null = null;\n  private subscriptions: Map<string, Set<string>> = new Map();\n  private retryConfig: RetryConfig;\n  \n  constructor(config: CoinbaseConfig) {\n    this.apiKey = process.env.COINBASE_API_KEY || '';\n    this.apiSecret = process.env.COINBASE_API_SECRET || '';\n    this.retryConfig = config.retryConfig || {\n      maxRetries: 5,\n      backoffMultiplier: 2,\n      maxBackoffSeconds: 60\n    };\n  }\n  \n  async connect(): Promise<void> {\n    if (!this.apiKey || !this.apiSecret) {\n      throw new Error('Coinbase API credentials not found in environment variables');\n    }\n    // Initialize REST client\n  }\n  \n  async disconnect(): Promise<void> {\n    // Close WebSocket connection if open\n    if (this.wsClient) {\n      this.wsClient.close();\n      this.wsClient = null;\n    }\n  }\n  \n  async *getHistoricalData(params: HistoricalParams): AsyncIterator<OhlcvDto> {\n    // Fetch historical data from REST API\n    // Handle pagination\n    // Apply rate limiting\n    // Convert to OhlcvDto format\n    // Yield bars in chronological order\n  }\n  \n  async *subscribeRealtime(params: RealtimeParams): AsyncIterator<OhlcvDto> {\n    // Connect to WebSocket if not already connected\n    await this.ensureWebSocketConnection();\n    \n    // Subscribe to channels for requested symbols\n    for (const symbol of params.symbols) {\n      await this.subscribeToSymbol(symbol, params.timeframe);\n    }\n    \n    // Create and return async iterator that yields real-time bars\n    // Handle reconnections and backfill gaps\n  }\n  \n  getRequiredEnvVars(): string[] {\n    return ['COINBASE_API_KEY', 'COINBASE_API_SECRET'];\n  }\n  \n  private async ensureWebSocketConnection(): Promise<void> {\n    // Create WebSocket connection if not exists\n    // Set up event handlers\n    // Implement reconnection logic\n  }\n  \n  private async subscribeToSymbol(symbol: string, timeframe: string): Promise<void> {\n    // Send subscription message to WebSocket\n    // Track active subscriptions\n  }\n  \n  private async backfill(symbol: string, lastTimestamp: number): Promise<OhlcvDto[]> {\n    // Calculate gap between last timestamp and current time\n    // Fetch missing data via REST API\n    // Return backfilled bars\n  }\n}\n```",
        "testStrategy": "1. Unit tests with mocked API responses\n2. Test rate limiting behavior\n3. Test reconnection logic with simulated disconnections\n4. Verify backfill mechanism works correctly\n5. Test authentication error handling\n6. Integration tests with actual Coinbase API (using test credentials)",
        "priority": "medium",
        "dependencies": [
          1,
          2
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 7,
        "title": "Technical Indicators Implementation",
        "description": "Implement technical indicators and advanced transformations including moving averages, RSI, Bollinger Bands, and more.",
        "details": "1. Create base class for technical indicators\n2. Implement `MovingAverage` transform (SMA, EMA)\n3. Implement `RSI` transform\n4. Implement `BollingerBands` transform\n5. Implement `MACD` transform\n6. Implement `ATR` transform\n7. Implement `VWAP` transform\n8. Ensure all indicators can use any column as source\n\n```typescript\nabstract class IndicatorTransform extends BaseTransform {\n  protected source: string;\n  protected outputColumn: string;\n  \n  constructor(type: TransformType, source: string = 'close', outputColumn?: string) {\n    super(type);\n    this.source = source;\n    this.outputColumn = outputColumn || `${type}_${this.getDefaultSuffix()}`;\n  }\n  \n  abstract getDefaultSuffix(): string;\n}\n\nclass MovingAverageTransform extends IndicatorTransform {\n  private type: 'sma' | 'ema';\n  private period: number;\n  private values: Map<string, number[]> = new Map();\n  \n  constructor(params: MovingAverageParams) {\n    super('movingAverage', params.source, params.outputColumn);\n    this.type = params.type;\n    this.period = params.period;\n  }\n  \n  getDefaultSuffix(): string {\n    return `${this.type}_${this.period}`;\n  }\n  \n  async *apply(data: AsyncIterator<OhlcvDto>): AsyncIterator<OhlcvDto> {\n    for await (const bar of data) {\n      const symbol = bar.symbol;\n      const value = bar[this.source] as number;\n      \n      if (!this.values.has(symbol)) {\n        this.values.set(symbol, []);\n      }\n      \n      const values = this.values.get(symbol)!;\n      values.push(value);\n      \n      // Keep only necessary history\n      if (values.length > this.period) {\n        values.shift();\n      }\n      \n      // Calculate MA based on type\n      let result: number | null = null;\n      if (values.length === this.period) {\n        if (this.type === 'sma') {\n          result = values.reduce((sum, val) => sum + val, 0) / this.period;\n        } else if (this.type === 'ema') {\n          // EMA calculation\n          // ...\n        }\n      }\n      \n      // Create new bar with indicator value\n      yield { ...bar, [this.outputColumn]: result };\n    }\n  }\n}\n\nclass RSITransform extends IndicatorTransform {\n  private period: number;\n  private prevValues: Map<string, number> = new Map();\n  private gains: Map<string, number[]> = new Map();\n  private losses: Map<string, number[]> = new Map();\n  \n  constructor(params: RSIParams) {\n    super('rsi', params.source, params.outputColumn);\n    this.period = params.period;\n  }\n  \n  getDefaultSuffix(): string {\n    return `${this.period}`;\n  }\n  \n  async *apply(data: AsyncIterator<OhlcvDto>): AsyncIterator<OhlcvDto> {\n    // RSI implementation\n    // ...\n  }\n}\n```",
        "testStrategy": "1. Unit tests for each indicator with known input/output values\n2. Test edge cases (insufficient data, zero values, etc.)\n3. Verify calculations match standard implementations\n4. Test with different source columns\n5. Benchmark performance with large datasets\n6. Verify memory usage remains constant during processing",
        "priority": "medium",
        "dependencies": [
          1,
          4
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 8,
        "title": "Alternative Bar Generation and Server Mode",
        "description": "Implement alternative bar types (tick, volume, dollar, tick-imbalance) and continuous server mode with backfill.",
        "details": "1. Implement `TickBarGenerator` transform\n2. Implement `VolumeBarGenerator` transform\n3. Implement `DollarBarGenerator` transform\n4. Implement `TickImbalanceBarGenerator` transform\n5. Implement `HeikinAshiGenerator` transform\n6. Create server mode for continuous operation\n7. Implement gap detection and backfill mechanism\n8. Add graceful shutdown with state persistence\n\n```typescript\nabstract class BarGeneratorTransform extends BaseTransform {\n  protected symbolState: Map<string, any> = new Map();\n  \n  constructor(type: TransformType) {\n    super(type);\n  }\n  \n  abstract isBarComplete(symbol: string, bar: OhlcvDto): boolean;\n  abstract createNewBar(symbol: string, bar: OhlcvDto): OhlcvDto;\n  abstract updateBar(symbol: string, currentBar: OhlcvDto, newBar: OhlcvDto): OhlcvDto;\n  \n  async *apply(data: AsyncIterator<OhlcvDto>): AsyncIterator<OhlcvDto> {\n    for await (const bar of data) {\n      const symbol = bar.symbol;\n      \n      if (!this.symbolState.has(symbol)) {\n        this.symbolState.set(symbol, {\n          currentBar: this.createNewBar(symbol, bar),\n          complete: false\n        });\n        continue;\n      }\n      \n      const state = this.symbolState.get(symbol)!;\n      state.currentBar = this.updateBar(symbol, state.currentBar, bar);\n      \n      if (this.isBarComplete(symbol, bar)) {\n        // Emit completed bar\n        yield state.currentBar;\n        // Start new bar\n        state.currentBar = this.createNewBar(symbol, bar);\n      }\n    }\n    \n    // Emit any final incomplete bars\n    for (const [symbol, state] of this.symbolState.entries()) {\n      if (!state.complete) {\n        yield state.currentBar;\n      }\n    }\n  }\n}\n\nclass VolumeBarGenerator extends BarGeneratorTransform {\n  private volumePerBar: number;\n  private accumulatedVolume: Map<string, number> = new Map();\n  \n  constructor(params: VolumeBarsParams) {\n    super('volumeBars');\n    this.volumePerBar = params.volumePerBar;\n  }\n  \n  isBarComplete(symbol: string, bar: OhlcvDto): boolean {\n    const accVolume = (this.accumulatedVolume.get(symbol) || 0) + bar.volume;\n    this.accumulatedVolume.set(symbol, accVolume);\n    \n    return accVolume >= this.volumePerBar;\n  }\n  \n  createNewBar(symbol: string, bar: OhlcvDto): OhlcvDto {\n    this.accumulatedVolume.set(symbol, bar.volume);\n    return { ...bar };\n  }\n  \n  updateBar(symbol: string, currentBar: OhlcvDto, newBar: OhlcvDto): OhlcvDto {\n    return {\n      ...currentBar,\n      high: Math.max(currentBar.high, newBar.high),\n      low: Math.min(currentBar.low, newBar.low),\n      close: newBar.close,\n      volume: currentBar.volume + newBar.volume,\n      timestamp: newBar.timestamp // Use timestamp of last update\n    };\n  }\n}\n\nclass Pipeline {\n  private provider: DataProvider;\n  private transforms: Transform[];\n  private repository: OhlcvRepository;\n  private options: PipelineOptions;\n  \n  constructor(provider: DataProvider, transforms: Transform[], repository: OhlcvRepository, options: PipelineOptions) {\n    this.provider = provider;\n    this.transforms = transforms;\n    this.repository = repository;\n    this.options = options;\n  }\n  \n  async run(): Promise<void> {\n    // Run pipeline once and exit\n    await this.provider.connect();\n    \n    try {\n      // Get historical data\n      let data = this.provider.getHistoricalData(this.options.historicalParams);\n      \n      // Apply transforms\n      for (const transform of this.transforms) {\n        data = transform.apply(data);\n      }\n      \n      // Process and store results\n      await this.processResults(data);\n    } finally {\n      await this.provider.disconnect();\n      await this.repository.close();\n    }\n  }\n  \n  async runContinuously(): Promise<void> {\n    // Run in server mode\n    await this.provider.connect();\n    await this.repository.initialize();\n    \n    try {\n      // Check for gaps and backfill if needed\n      if (this.options.backfillOnStartup) {\n        await this.backfill();\n      }\n      \n      // Subscribe to real-time data\n      let data = this.provider.subscribeRealtime(this.options.realtimeParams);\n      \n      // Apply transforms\n      for (const transform of this.transforms) {\n        data = transform.apply(data);\n      }\n      \n      // Process and store results continuously\n      await this.processResults(data);\n    } finally {\n      await this.provider.disconnect();\n      await this.repository.close();\n    }\n  }\n  \n  private async backfill(): Promise<void> {\n    // Get last timestamp from repository\n    // Calculate gap\n    // Fetch missing data\n    // Process through pipeline\n    // Store results\n  }\n  \n  private async processResults(data: AsyncIterator<OhlcvDto>): Promise<void> {\n    // Process in batches\n    const batchSize = this.options.batchSize || 1000;\n    let batch: OhlcvDto[] = [];\n    \n    for await (const bar of data) {\n      batch.push(bar);\n      \n      if (batch.length >= batchSize) {\n        await this.repository.appendBatch(batch);\n        batch = [];\n      }\n    }\n    \n    // Store any remaining items\n    if (batch.length > 0) {\n      await this.repository.appendBatch(batch);\n    }\n    \n    // Store transform coefficients\n    for (const transform of this.transforms) {\n      const coefficients = transform.getCoefficients();\n      if (coefficients) {\n        // Store coefficients in repository\n      }\n    }\n  }\n}\n```",
        "testStrategy": "1. Unit tests for each bar generator\n2. Test with various threshold values\n3. Verify bar properties are correctly calculated\n4. Test server mode with simulated real-time data\n5. Verify gap detection and backfill mechanism\n6. Test graceful shutdown and restart\n7. Integration tests with full pipeline in server mode",
        "priority": "low",
        "dependencies": [
          1,
          4,
          5,
          6
        ],
        "status": "pending",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-07-04T20:49:04.828Z",
      "updated": "2025-07-04T21:01:38.134Z",
      "description": "Tasks for master context"
    }
  }
}